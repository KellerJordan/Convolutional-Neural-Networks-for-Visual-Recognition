{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class ChunkSampler(sampler.Sampler):\n",
    "    \"\"\"Samples elements sequentially from some offset. \n",
    "    Arguments:\n",
    "        num_samples: # of desired datapoints\n",
    "        start: offset where we should start selecting from\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, start = 0):\n",
    "        self.num_samples = num_samples\n",
    "        self.start = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.start + self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "NUM_TRAIN = 49000\n",
    "NUM_VAL = 1000\n",
    "\n",
    "cifar10_train = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                           transform=T.ToTensor())\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, sampler=ChunkSampler(NUM_TRAIN, 0))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                           transform=T.ToTensor())\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, sampler=ChunkSampler(NUM_VAL, NUM_TRAIN))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./cs231n/datasets', train=False, download=True,\n",
    "                          transform=T.ToTensor())\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)\n",
    "\n",
    "dtype = torch.FloatTensor # the CPU datatype\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "# This is a little utility that we'll use to reset the model\n",
    "# if we want to re-initialize all our parameters\n",
    "def reset(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() # read in N, C, H, W\n",
    "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "gpu_dtype = torch.cuda.FloatTensor\n",
    "def train(model, loss_fn, optimizer, num_epochs = 1):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
    "        model.train()\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            x_var = Variable(x.type(gpu_dtype))\n",
    "            y_var = Variable(y.type(gpu_dtype).long())\n",
    "\n",
    "            scores = model(x_var)\n",
    "            \n",
    "            loss = loss_fn(scores, y_var)\n",
    "            if (t + 1) % print_every == 0:\n",
    "                print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def check_accuracy(model, loader):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    for x, y in loader:\n",
    "        x_var = Variable(x.type(gpu_dtype), volatile=True)\n",
    "\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_samples += preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormReLU(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(NormReLU, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(num_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_bn = self.bn(x)\n",
    "        return self.relu(h_bn)\n",
    "\n",
    "# def NormRelu(num_features):\n",
    "#     return nn.Sequential(\n",
    "#         nn.BatchNorm2d(num_features),\n",
    "#         nn.ReLU(inplace=True)\n",
    "#     )\n",
    "# NormRelu = nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 20\n",
      "t = 100, loss = 1.3083\n",
      "t = 200, loss = 1.1963\n",
      "t = 300, loss = 0.9826\n",
      "t = 400, loss = 0.7789\n",
      "t = 500, loss = 0.7385\n",
      "t = 600, loss = 0.8927\n",
      "t = 700, loss = 1.1731\n",
      "Starting epoch 2 / 20\n",
      "t = 100, loss = 0.5525\n",
      "t = 200, loss = 0.7502\n",
      "t = 300, loss = 0.7011\n",
      "t = 400, loss = 0.5488\n",
      "t = 500, loss = 0.5556\n",
      "t = 600, loss = 0.5953\n",
      "t = 700, loss = 0.8116\n",
      "Starting epoch 3 / 20\n",
      "t = 100, loss = 0.3301\n",
      "t = 200, loss = 0.5374\n",
      "t = 300, loss = 0.4179\n",
      "t = 400, loss = 0.3307\n",
      "t = 500, loss = 0.3622\n",
      "t = 600, loss = 0.3075\n",
      "t = 700, loss = 0.5627\n",
      "Starting epoch 4 / 20\n",
      "t = 100, loss = 0.2026\n",
      "t = 200, loss = 0.2604\n",
      "t = 300, loss = 0.2310\n",
      "t = 400, loss = 0.1658\n",
      "t = 500, loss = 0.1967\n",
      "t = 600, loss = 0.2934\n",
      "t = 700, loss = 0.3703\n",
      "Starting epoch 5 / 20\n",
      "t = 100, loss = 0.0918\n",
      "t = 200, loss = 0.0783\n",
      "t = 300, loss = 0.0881\n",
      "t = 400, loss = 0.1317\n",
      "t = 500, loss = 0.1243\n",
      "t = 600, loss = 0.1869\n",
      "t = 700, loss = 0.3971\n",
      "Starting epoch 6 / 20\n",
      "t = 100, loss = 0.1143\n",
      "t = 200, loss = 0.1283\n",
      "t = 300, loss = 0.1537\n",
      "t = 400, loss = 0.0811\n",
      "t = 500, loss = 0.0832\n",
      "t = 600, loss = 0.1333\n",
      "t = 700, loss = 0.0078\n",
      "Starting epoch 7 / 20\n",
      "t = 100, loss = 0.0342\n",
      "t = 200, loss = 0.0889\n",
      "t = 300, loss = 0.1591\n",
      "t = 400, loss = 0.0253\n",
      "t = 500, loss = 0.0462\n",
      "t = 600, loss = 0.0467\n",
      "t = 700, loss = 0.0864\n",
      "Starting epoch 8 / 20\n",
      "t = 100, loss = 0.0872\n",
      "t = 200, loss = 0.0500\n",
      "t = 300, loss = 0.0166\n",
      "t = 400, loss = 0.0492\n",
      "t = 500, loss = 0.0097\n",
      "t = 600, loss = 0.0056\n",
      "t = 700, loss = 0.1545\n",
      "Starting epoch 9 / 20\n",
      "t = 100, loss = 0.0158\n",
      "t = 200, loss = 0.0654\n",
      "t = 300, loss = 0.0607\n",
      "t = 400, loss = 0.0147\n",
      "t = 500, loss = 0.0533\n",
      "t = 600, loss = 0.0516\n",
      "t = 700, loss = 0.0109\n",
      "Starting epoch 10 / 20\n",
      "t = 100, loss = 0.0234\n",
      "t = 200, loss = 0.0170\n",
      "t = 300, loss = 0.0487\n",
      "t = 400, loss = 0.0090\n",
      "t = 500, loss = 0.0188\n",
      "t = 600, loss = 0.0207\n",
      "t = 700, loss = 0.0131\n",
      "Starting epoch 11 / 20\n",
      "t = 100, loss = 0.0742\n",
      "t = 200, loss = 0.0082\n",
      "t = 300, loss = 0.0476\n",
      "t = 400, loss = 0.0535\n",
      "t = 500, loss = 0.0408\n",
      "t = 600, loss = 0.1110\n",
      "t = 700, loss = 0.0082\n",
      "Starting epoch 12 / 20\n",
      "t = 100, loss = 0.0498\n",
      "t = 200, loss = 0.0312\n",
      "t = 300, loss = 0.0012\n",
      "t = 400, loss = 0.1125\n",
      "t = 500, loss = 0.0225\n",
      "t = 600, loss = 0.0160\n",
      "t = 700, loss = 0.1096\n",
      "Starting epoch 13 / 20\n",
      "t = 100, loss = 0.0580\n",
      "t = 200, loss = 0.0445\n",
      "t = 300, loss = 0.0651\n",
      "t = 400, loss = 0.0122\n",
      "t = 500, loss = 0.1397\n",
      "t = 600, loss = 0.0709\n",
      "t = 700, loss = 0.0387\n",
      "Starting epoch 14 / 20\n",
      "t = 100, loss = 0.0111\n",
      "t = 200, loss = 0.0075\n",
      "t = 300, loss = 0.0025\n",
      "t = 400, loss = 0.0108\n",
      "t = 500, loss = 0.0186\n",
      "t = 600, loss = 0.0022\n",
      "t = 700, loss = 0.0285\n",
      "Starting epoch 15 / 20\n",
      "t = 100, loss = 0.0251\n",
      "t = 200, loss = 0.0515\n",
      "t = 300, loss = 0.0657\n",
      "t = 400, loss = 0.0097\n",
      "t = 500, loss = 0.0609\n",
      "t = 600, loss = 0.0861\n",
      "t = 700, loss = 0.0240\n",
      "Starting epoch 16 / 20\n",
      "t = 100, loss = 0.0385\n",
      "t = 200, loss = 0.0185\n",
      "t = 300, loss = 0.0017\n",
      "t = 400, loss = 0.0070\n",
      "t = 500, loss = 0.0112\n",
      "t = 600, loss = 0.0009\n",
      "t = 700, loss = 0.0027\n",
      "Starting epoch 17 / 20\n",
      "t = 100, loss = 0.0048\n",
      "t = 200, loss = 0.0045\n",
      "t = 300, loss = 0.0039\n",
      "t = 400, loss = 0.0055\n",
      "t = 500, loss = 0.0064\n",
      "t = 600, loss = 0.0015\n",
      "t = 700, loss = 0.0117\n",
      "Starting epoch 18 / 20\n",
      "t = 100, loss = 0.0310\n",
      "t = 200, loss = 0.0340\n",
      "t = 300, loss = 0.0100\n",
      "t = 400, loss = 0.0185\n",
      "t = 500, loss = 0.0059\n",
      "t = 600, loss = 0.0007\n",
      "t = 700, loss = 0.0052\n",
      "Starting epoch 19 / 20\n",
      "t = 100, loss = 0.0084\n",
      "t = 200, loss = 0.0795\n",
      "t = 300, loss = 0.0393\n",
      "t = 400, loss = 0.0039\n",
      "t = 500, loss = 0.0723\n",
      "t = 600, loss = 0.0011\n",
      "t = 700, loss = 0.0219\n",
      "Starting epoch 20 / 20\n",
      "t = 100, loss = 0.0120\n",
      "t = 200, loss = 0.0023\n",
      "t = 300, loss = 0.0090\n",
      "t = 400, loss = 0.0018\n",
      "t = 500, loss = 0.0829\n",
      "t = 600, loss = 0.0716\n",
      "t = 700, loss = 0.0153\n",
      "Checking accuracy on validation set\n",
      "Got 809 / 1000 correct (80.90)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "    NormReLU(64),\n",
    "    nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "    NormReLU(64),\n",
    "    nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "    NormReLU(64),\n",
    "    nn.MaxPool2d(2, stride=2),\n",
    "    nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "    NormReLU(64),\n",
    "    nn.MaxPool2d(2, stride=2),\n",
    "    Flatten(),\n",
    "    nn.Linear(4096, 1024),\n",
    "    NormReLU(1024),\n",
    "    nn.Linear(1024, 1024),\n",
    "    NormReLU(1024),\n",
    "    nn.Linear(1024, 10),\n",
    ").type(gpu_dtype)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().type(gpu_dtype)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "train(model, loss_fn, optimizer, num_epochs=20)\n",
    "check_accuracy(model, loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 5\n",
      "t = 100, loss = 1.2715\n",
      "t = 200, loss = 1.2823\n",
      "t = 300, loss = 1.1503\n",
      "t = 400, loss = 0.7735\n",
      "t = 500, loss = 0.9118\n",
      "t = 600, loss = 0.8282\n",
      "t = 700, loss = 1.0795\n",
      "Starting epoch 2 / 5\n",
      "t = 100, loss = 0.6075\n",
      "t = 200, loss = 0.7615\n",
      "t = 300, loss = 0.8118\n",
      "t = 400, loss = 0.6404\n",
      "t = 500, loss = 0.6070\n",
      "t = 600, loss = 0.4910\n",
      "t = 700, loss = 0.7292\n",
      "Starting epoch 3 / 5\n",
      "t = 100, loss = 0.3637\n",
      "t = 200, loss = 0.5214\n",
      "t = 300, loss = 0.6097\n",
      "t = 400, loss = 0.5482\n",
      "t = 500, loss = 0.4808\n",
      "t = 600, loss = 0.3414\n",
      "t = 700, loss = 0.5616\n",
      "Starting epoch 4 / 5\n",
      "t = 100, loss = 0.2474\n",
      "t = 200, loss = 0.4462\n",
      "t = 300, loss = 0.4560\n",
      "t = 400, loss = 0.3813\n",
      "t = 500, loss = 0.3232\n",
      "t = 600, loss = 0.2337\n",
      "t = 700, loss = 0.4190\n",
      "Starting epoch 5 / 5\n",
      "t = 100, loss = 0.1474\n",
      "t = 200, loss = 0.2500\n",
      "t = 300, loss = 0.3246\n",
      "t = 400, loss = 0.2570\n",
      "t = 500, loss = 0.2150\n",
      "t = 600, loss = 0.1830\n",
      "t = 700, loss = 0.2991\n",
      "Checking accuracy on validation set\n",
      "Got 736 / 1000 correct (73.60)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "    NormReLU(64),\n",
    "    nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "    NormReLU(64),\n",
    "    nn.MaxPool2d(2, stride=2),\n",
    "    nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "    NormReLU(64),\n",
    "    nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "    NormReLU(64),\n",
    "    nn.MaxPool2d(2, stride=2),\n",
    "    nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "    NormReLU(128),\n",
    "    nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "    NormReLU(128),\n",
    "    nn.MaxPool2d(2, stride=2),\n",
    "    Flatten(),\n",
    "    nn.Linear(2048, 1024),\n",
    "    NormReLU(1024),\n",
    "    nn.Linear(1024, 1024),\n",
    "    NormReLU(1024),\n",
    "    nn.Linear(1024, 10),\n",
    ").type(gpu_dtype)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().type(gpu_dtype)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "train(model, loss_fn, optimizer, num_epochs=5)\n",
    "check_accuracy(model, loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_filters, channels_in=None, stride=1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.downsample = stride != 1\n",
    "        if self.downsample:\n",
    "            self.conv1x1 = nn.Conv2d(channels_in, num_filters,\n",
    "                                     kernel_size=1, stride=stride)\n",
    "        if not channels_in:\n",
    "            channels_in = num_filters\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channels_in, num_filters,\n",
    "                               kernel_size=3, stride=1, padding=1)\n",
    "        self.bnrelu1 = NormReLU(num_filters)\n",
    "        self.conv2 = nn.Conv2d(num_filters, num_filters,\n",
    "                              kernel_size=3, stride=stride, padding=1)\n",
    "        self.bnrelu2 = NormReLU(num_filters)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.conv2(self.bnrelu1(self.conv1(x)))\n",
    "        if self.downsample:\n",
    "#             print(self.conv1x1(x).size(), h.size())\n",
    "            preact = h + self.conv1x1(x)\n",
    "        else:\n",
    "            preact = h + x\n",
    "        out = self.bnrelu2(preact)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([256, 128, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 128, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([512, 256, 1, 1])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 256, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([10, 512])\n",
      "torch.Size([10])\n",
      "Sequential(\n",
      "  (0): Conv2d (3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): NormReLU(\n",
      "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu): ReLU(inplace)\n",
      "  )\n",
      "  (2): ResBlock(\n",
      "    (conv1): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu1): NormReLU(\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu2): NormReLU(\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (3): ResBlock(\n",
      "    (conv1): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu1): NormReLU(\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu2): NormReLU(\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (4): ResBlock(\n",
      "    (conv1): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu1): NormReLU(\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu2): NormReLU(\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (5): ResBlock(\n",
      "    (conv1): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu1): NormReLU(\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu2): NormReLU(\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (6): ResBlock(\n",
      "    (conv1x1): Conv2d (128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "    (conv1): Conv2d (128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu1): NormReLU(\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (conv2): Conv2d (256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bnrelu2): NormReLU(\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (7): ResBlock(\n",
      "    (conv1): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu1): NormReLU(\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (conv2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu2): NormReLU(\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (8): ResBlock(\n",
      "    (conv1): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu1): NormReLU(\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (conv2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu2): NormReLU(\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (9): ResBlock(\n",
      "    (conv1): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu1): NormReLU(\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (conv2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu2): NormReLU(\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (10): ResBlock(\n",
      "    (conv1x1): Conv2d (256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "    (conv1): Conv2d (256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu1): NormReLU(\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (conv2): Conv2d (512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bnrelu2): NormReLU(\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (11): ResBlock(\n",
      "    (conv1): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu1): NormReLU(\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (conv2): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu2): NormReLU(\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (12): ResBlock(\n",
      "    (conv1): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu1): NormReLU(\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (conv2): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu2): NormReLU(\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (13): ResBlock(\n",
      "    (conv1): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu1): NormReLU(\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (conv2): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bnrelu2): NormReLU(\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (14): AvgPool2d(kernel_size=8, stride=8, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  (15): Flatten(\n",
      "  )\n",
      "  (16): Linear(in_features=512, out_features=10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=1),\n",
    "    NormReLU(128),\n",
    "    ResBlock(128),\n",
    "    ResBlock(128),\n",
    "    ResBlock(128),\n",
    "    ResBlock(128),\n",
    "    ResBlock(256, channels_in=128, stride=2),\n",
    "    ResBlock(256),\n",
    "    ResBlock(256),\n",
    "    ResBlock(256),\n",
    "    ResBlock(512, channels_in=256, stride=2),\n",
    "    ResBlock(512),\n",
    "    ResBlock(512),\n",
    "    ResBlock(512),\n",
    "    nn.AvgPool2d(8),\n",
    "    Flatten(),\n",
    "    nn.Linear(512, 10),\n",
    ").type(gpu_dtype)\n",
    "\n",
    "# total = 0\n",
    "for p in model.parameters():\n",
    "    print(p.size())\n",
    "# print(model.parameters())\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().type(gpu_dtype)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# train(model, loss_fn, optimizer, num_epochs=2)\n",
    "# check_accuracy(model, loader_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
